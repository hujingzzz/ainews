---
layout: post
title: DeepSeek ReleaseDeepSeek R1-0528-Significant improvement in code and writing skillsA deeper sense of reasoning. I've also been able to fix some problems.
date: 2025-07-09 12:00:00 +0800
category: Frontier Trends
thumbnail: /style/image/deepseek-deepseek-r1-0528_1.jpg
icon: book
---
* content
{:toc}

DeepSeek Release**DeepSeek R1-0528Update **DeepSeek R1-0528Update Last night, the micro-technology community was informed of this update.‚ÄúAt the end of the day,**Small-scale experimental upgrade**Small-scale experimental upgradeI'm not sure what I'm going to do.It's been open for testing.I don't know.

## **DeepSeek R1-0528 Update Highlights**DeepSeek R1-0528 Update Highlights

### 1. The ability to think in depth has increased dramatically.
DeepSeek-R1-0528 Use Still 2024 Year 12 It's published in the month. DeepSeek V3 Base Model as base But in the training, more money was put into it. It significantly enhances the depth of thinking and reasoning of the model.I don't know.
Updated R1 The model's in math.Several baseline assessments, such as programming and universal logic, have achieved the highest performance in all models currently in the country. And they're close to other international best models in their overall performance. Like o3 and Gemini-2.5-ProI don't know.
![](https://assets-v2.circle.so/vuglp8rxg0u64wag0atpvqle7cbo)
- **Technical background**Technical background-‚ô™ Though still based on ‚ô™ DeepSeek V3 Base But this time, arithmetic resource investment in the process of post-enhanced training ‚ô™ And the model's on ‚ô™**Visible enhancement in the chain of reasoning**Visible enhancement in the chain of reasoningI don't know.

- **AIME 2025 Monitoring of evaluation results**AIME 2025 Monitoring of evaluation results-
Accuracy of old editions-70%

- Accuracy of new editions-**87.5%**87.5%

**Token Use Contrast**Token Use ContrastEvery Question:

- Old edition-12K

- New edition-23Küëâ Show new editions of energy‚ÄúAt the end of the day,Think a little slower and deeper.I'm not sure what I'm going to do.Simulate the path of detailed human reasoningI don't know.

### 2. Think chain distillation to medium-sized models
Through distillation DeepSeek-R1-0528 ...the training behind the chain of thought. Qwen3-8B Base Got it. DeepSeek-R1-0528-Qwen3-8BI don't know.The... 8B Models are being tested for mathematics. AIME 2024 is second only to DeepSeek-R1-0528 Beyond Qwen3-8B +10.0%This is the first time I've ever seen you.and Qwen3-235B Quite.I don't know.**We believe that. DeepSeek-R1-0528 The chain of thought will be important both for the study of academic reasoning models and for the development of small models by industry.I don't know.**We believe that. DeepSeek-R1-0528 The chain of thought will be important both for the study of academic reasoning models and for the development of small models by industry.I don't know.

- Based on R1-0528 Distillation **Qwen3-8B Base Version**Qwen3-8B Base VersionI don't know.

- The new model is... AIME 2024 The neutral is better than the neutral. Qwen3-8B Close. Qwen3-235BI don't know.

- Shows the distillation.‚ÄúAt the end of the day,The thought chain.It's not a good idea, it's not a good idea.High academic and industrial value.I don't know.
![](https://assets-v2.circle.so/4cexjbe4m88sxtx2f6vvsuy500ix)

### 3.Increased writing capacity

- Production capacity was fine-tuned and optimizedI don't know.

- Support for more complete structureStyles are closer to the long text output of humansI don't know.

- In particular.**PapersNovel.Complicated styles like essays**PapersNovel.Complicated styles like essaysSubstantive increase in output capacityI don't know.
![](https://assets-v2.circle.so/6mzro4d69k59nldzrm3ifow3qg3s)

### 4.Error Control and‚ÄúAt the end of the day,Illusion RateIt's not a good idea, it's not a good idea.Decline

- **Illustrated target.**Illustrated target.-Reducing model output‚ÄúAt the end of the day,There's nothing there.It's not a good idea, it's not a good idea.ContentsI don't know.

- **Applied scene**Applied scene-
Rewrite Liquefied

- Read to understand.

- Summary

**Performance enhancement data**Performance enhancement data-

- The hallucination rate's down.-**- I'm sorry. - I'm sorry.45%~50%**- I'm sorry. - I'm sorry.45%~50%

- Output is more credible Increased consistency of information

### 5.Tool call and code enhancement

- New version support **Function Calling**Function Calling and **JsonOutput**JsonOutputI don't know.

- **Tau-BenchAssessment of achievements**Tau-BenchAssessment of achievements-
Airline: 53.5%

- Retail: 63.9%

- Horizontally close. OpenAI o1-high But it's a bit less than that. o3-highClaude 4 SonnetI don't know.

![](https://assets-v2.circle.so/e1l6gsyx43pif5lhj3zgsj4dvc0w)**Upgrade of front-end code generation capability**Upgrade of front-end code generation capability ‚ô™ And raise in ‚ô™‚ÄúAt the end of the day,Role-playing.It's not a good idea, it's not a good idea.In the complex task of multiple rounds of dialogueI don't know.
![](https://assets-v2.circle.so/bxpxjz9ciei6lh55cubt38kevpxp)
### 6Ô∏è‚É£ üõ† **Problem restoration and optimization**Problem restoration and optimization

- Rehabilitate early R1 Common problems of models-
**Repeat the answer.**Repeat the answer.Problem

- **Format Confusion**Format Confusion

- **Language mixC-E-MultipleI'm not sure what I'm talking about.**Language mixC-E-MultipleI'm not sure what I'm talking about.

Output More Standard **Suitable for professional settings**Suitable for professional settingsI don't know.

### Performance in multi-task capacity optimization
![](https://assets-v2.circle.so/gtq5ems4nbbww7f0el57rexhbefe)
- **Benchmark Evaluation**Benchmark Evaluation-‚ô™ In the cause ‚ô™ UC BerkeleyMIT and Cornell Developed **LiveCodeBench**LiveCodeBench Code Generation Evaluation-
R1-0528 The ability to reason with **OpenAI It's... it's... it's... o3**OpenAI It's... it's... it's... o3When the front-line model works equally, Slightly below**o4 mini**o4 mini Especially in math.It's an excellent programming and complex reasoning mission.I don't know.

- **Better than xAI It's... it's... it's... Grok 3 mini And Ali Baba's. Qwen 3**Better than xAI It's... it's... it's... Grok 3 mini And Ali Baba's. Qwen 3I don't know.

The reasoning is more thought-provoking. Chain-based reasoningChain-of-Thought, CoTI'm not sure what I'm talking about.A clearer structure. Output format is more naturalI don't know.
This achievement has been further enhanced. DeepSeek In Central America. AI Impact of technology competitionI don't know.
![](https://assets-v2.circle.so/io4j1766fe2nu4razu7azekgbtz0)DeepSeekAlready R1-0528 Upload to Hugging Face No public description or model description has yet been publishedI don't know.

## **API Update**API Update**‚Äã**‚Äã
API Synchronized Updated Interfaces and calls will remain unchangedI don't know.New edition R1 API Still supports looking at the model thinking process It's also increased. Function Calling and JsonOutput SupportI don't know.
We're on the new edition. R1 API Medium max_tokens The parameters have been adjusted.-Now. max_tokensTo limit the total length of a single output of a modelIncluding the reflection process.This is the first time I've ever seen you.Default As 32K Max. of 64KI don't know.Please. API Users adjust in a timely manner max_tokens Parameters in case output is cut in advanceI don't know.
R1 Details of how the model was used API Guide-https://api-docs.deepseek.com/zh-cn/guides/reasoning_modelI don't know.
This one. R1 Updated Official websiteAppletApp End and API in which the background length of the model remains 64KI don't know.If the user needs a longer context length The context length can be called through other third-party platforms as 128K Open Source Version R1-0528 ModelI don't know.

## **Model Open Source**Model Open Source**‚Äã**‚Äã
DeepSeek-R1-0528 ‚ô™ With the old one ‚ô™ DeepSeek-R1 Use the same base Model Only post-training methods have been improvedI don't know.Pre-privatization deployment only needs to be updated checkpoint and tokenizer_config.jsontool calls Related changesI'm sorry, I don't know.Model Parameter is 685Bof which 14B Yes. MTP LayerThis is the first time I've ever seen you.Open Source Context Length 128KWebendApp and API Provision 64K ContextI'm sorry, I don't know.
DeepSeek-R1-0528 Please refer to model weight downloads-
**Model Scope:**Model Scope: https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528
**Huggingface:**Huggingface: https://huggingface.co/deepseek-ai/DeepSeek-R1-0528
with the old version DeepSeek-R1 Consistency This time, our open-source warehouse.Including model weightsI'm not sure what I'm talking about.It's still uniform. MIT License and allow the user to use the model for outputTrain other models, for example, through model distillation.I don't know.

## **Latest callDeepseek-R1-0528 API Website**Latest callDeepseek-R1-0528 API Website
1Openrouter Address-https://openrouter.ai
2. GMI tutt: https://inference-engine.gmicloud.ai
3. Novita tutt: https://novita.ai
4. Nebius tbtit: https://studio.nebius.com
5. Inference toti: https://inference.net 
Official presentation-https://api-docs.deepseek.com/zh-cn/news/news250528
