---
layout: post
title: DeepSeek ReleaseDeepSeek R1-0528# I don't know what I'm talking about
date: 2025-07-09 12:00:00 +0800
category: Frontier Trends
thumbnail: /style/image/deepseek-deepseek-r1-0528_1.jpg
icon: book
---
* content
{:toc}

DeepSeek Release**DeepSeek R1-0528UpdateI'm not sure what you're talking about.**DeepSeek R1-0528UpdateI'm not sure what you're talking about.Last night, the micro-technology community was informed of this update.‚ÄúAt the end of the day,**Small-scale experimental upgrade**Small-scale experimental upgradeI'm not sure what I'm going to do.It's been open for testing.I don't know.

## **DeepSeek R1-0528 Update Highlights**DeepSeek R1-0528 Update Highlights

### 1. The ability to think in depth has increased dramatically.
DeepSeek-R1-0528 Use Still 2024 Year 12 It's published in the month. DeepSeek V3 Base Model as baseI'm not sure what you're talking about.But in the training, more money was put into it.I'm not sure what you're talking about.It significantly enhances the depth of thinking and reasoning of the model.I don't know.
Updated R1 The model's in math.I don't know what you're talking about.Several baseline assessments, such as programming and universal logic, have achieved the highest performance in all models currently in the country.I'm not sure what you're talking about.And they're close to other international best models in their overall performance.I'm not sure what you're talking about.Like o3 and Gemini-2.5-ProI don't know.
![](https://assets-v2.circle.so/vuglp8rxg0u64wag0atpvqle7cbo)
- **Technical background**Technical background# I don't know what I'm talking about #‚ô™ Though still based on ‚ô™ DeepSeek V3 BaseI'm not sure what you're talking about.But this time, arithmetic resource investment in the process of post-enhanced trainingI'm not sure what you're talking about.‚ô™ And the model's on ‚ô™**Visible enhancement in the chain of reasoning**Visible enhancement in the chain of reasoningI don't know.

- **AIME 2025 Monitoring of evaluation results**AIME 2025 Monitoring of evaluation results# I don't know what I'm talking about #
Accuracy of old editions# I don't know what I'm talking about #70%

- Accuracy of new editions# I don't know what I'm talking about #**87.5%**87.5%

**Token Use Contrast**Token Use ContrastI don't know what you're talking about.Every Question:

- Old edition# I don't know what I'm talking about #12K

- New edition# I don't know what I'm talking about #23Küëâ Show new editions of energy‚ÄúAt the end of the day,Think a little slower and deeper.I'm not sure what I'm going to do.Simulate the path of detailed human reasoningI don't know.

### 2. Think chain distillation to medium-sized models
Through distillation DeepSeek-R1-0528 ...the training behind the chain of thought. Qwen3-8B BaseI'm not sure what you're talking about.Got it. DeepSeek-R1-0528-Qwen3-8BI don't know.The... 8B Models are being tested for mathematics. AIME 2024 is second only to DeepSeek-R1-0528I'm not sure what you're talking about.Beyond Qwen3-8B I don't know what you're talking about.+10.0%This is the first time I've ever seen you.and Qwen3-235B Quite.I don't know.**We believe that.I'm not sure what you're talking about.DeepSeek-R1-0528 The chain of thought will be important both for the study of academic reasoning models and for the development of small models by industry.I don't know.**We believe that.I'm not sure what you're talking about.DeepSeek-R1-0528 The chain of thought will be important both for the study of academic reasoning models and for the development of small models by industry.I don't know.

- Based on R1-0528 Distillation **Qwen3-8B Base Version**Qwen3-8B Base VersionI don't know.

- The new model is... AIME 2024 The neutral is better than the neutral. Qwen3-8BI'm not sure what you're talking about.Close. Qwen3-235BI don't know.

- Shows the distillation.‚ÄúAt the end of the day,The thought chain.It's not a good idea, it's not a good idea.High academic and industrial value.I don't know.
![](https://assets-v2.circle.so/4cexjbe4m88sxtx2f6vvsuy500ix)

### 3.Increased writing capacity

- Production capacity was fine-tuned and optimizedI don't know.

- Support for more complete structureI don't know what you're talking about.Styles are closer to the long text output of humansI don't know.

- In particular.**PapersI don't know what you're talking about.Novel.I don't know what you're talking about.Complicated styles like essays**PapersI don't know what you're talking about.Novel.I don't know what you're talking about.Complicated styles like essaysSubstantive increase in output capacityI don't know.
![](https://assets-v2.circle.so/6mzro4d69k59nldzrm3ifow3qg3s)

### 4.Error Control and‚ÄúAt the end of the day,Illusion RateIt's not a good idea, it's not a good idea.Decline

- **Illustrated target.**Illustrated target.# I don't know what I'm talking about #Reducing model output‚ÄúAt the end of the day,There's nothing there.It's not a good idea, it's not a good idea.ContentsI don't know.

- **Applied scene**Applied scene# I don't know what I'm talking about #
Rewrite Liquefied

- Read to understand.

- Summary

**Performance enhancement data**Performance enhancement data# I don't know what I'm talking about #

- The hallucination rate's down.# I don't know what I'm talking about #**- I'm sorry. - I'm sorry.45%~50%**- I'm sorry. - I'm sorry.45%~50%

- Output is more credibleI'm not sure what you're talking about.Increased consistency of information

### 5.Tool call and code enhancement

- New version support **Function Calling**Function Calling and **JsonOutput**JsonOutputI don't know.

- **Tau-BenchAssessment of achievements**Tau-BenchAssessment of achievements# I don't know what I'm talking about #
Airline: 53.5%

- Retail: 63.9%

- Horizontally close. OpenAI o1-highI'm not sure what you're talking about.But it's a bit less than that. o3-highI don't know what you're talking about.Claude 4 SonnetI don't know.

![](https://assets-v2.circle.so/e1l6gsyx43pif5lhj3zgsj4dvc0w)**Upgrade of front-end code generation capability**Upgrade of front-end code generation capabilityI'm not sure what you're talking about.‚ô™ And raise in ‚ô™‚ÄúAt the end of the day,Role-playing.It's not a good idea, it's not a good idea.In the complex task of multiple rounds of dialogueI don't know.
![](https://assets-v2.circle.so/bxpxjz9ciei6lh55cubt38kevpxp)
### 6Ô∏è‚É£ üõ† **Problem restoration and optimization**Problem restoration and optimization

- Rehabilitate early R1 Common problems of models# I don't know what I'm talking about #
**Repeat the answer.**Repeat the answer.Problem

- **Format Confusion**Format Confusion

- **Language mixI don't know what you're talking about.C-E-MultipleI'm not sure what I'm talking about.**Language mixI don't know what you're talking about.C-E-MultipleI'm not sure what I'm talking about.

Output More StandardI'm not sure what you're talking about.**Suitable for professional settings**Suitable for professional settingsI don't know.

### Performance in multi-task capacity optimization
![](https://assets-v2.circle.so/gtq5ems4nbbww7f0el57rexhbefe)
- **Benchmark Evaluation**Benchmark Evaluation# I don't know what I'm talking about #‚ô™ In the cause ‚ô™ UC BerkeleyI don't know what you're talking about.MIT and Cornell Developed **LiveCodeBench**LiveCodeBench Code Generation Evaluation# I don't know what I'm talking about #
R1-0528 The ability to reason with **OpenAI It's... it's... it's... o3**OpenAI It's... it's... it's... o3When the front-line model works equally,I'm not sure what you're talking about.Slightly below**o4 mini**o4 miniI'm not sure what you're talking about.Especially in math.I don't know what you're talking about.It's an excellent programming and complex reasoning mission.I don't know.

- **Better than xAI It's... it's... it's... Grok 3 mini And Ali Baba's. Qwen 3**Better than xAI It's... it's... it's... Grok 3 mini And Ali Baba's. Qwen 3I don't know.

The reasoning is more thought-provoking.I'm not sure what you're talking about.Chain-based reasoningI don't know what you're talking about.Chain-of-Thought, CoTI'm not sure what I'm talking about.A clearer structure.I'm not sure what you're talking about.Output format is more naturalI don't know.
This achievement has been further enhanced. DeepSeek In Central America. AI Impact of technology competitionI don't know.
![](https://assets-v2.circle.so/io4j1766fe2nu4razu7azekgbtz0)DeepSeekAlready R1-0528 Upload to Hugging FaceI'm not sure what you're talking about.No public description or model description has yet been publishedI don't know.

## **API Update**API Update**‚Äã**‚Äã
API Synchronized UpdatedI'm not sure what you're talking about.Interfaces and calls will remain unchangedI don't know.New edition R1 API Still supports looking at the model thinking processI'm not sure what you're talking about.It's also increased. Function Calling and JsonOutput SupportI don't know.
We're on the new edition. R1 API Medium max_tokens The parameters have been adjusted.# I don't know what I'm talking about #Now. max_tokensTo limit the total length of a single output of a modelI don't know what you're talking about.Including the reflection process.This is the first time I've ever seen you.Default As 32KI'm not sure what you're talking about.Max. of 64KI don't know.Please. API Users adjust in a timely manner max_tokens Parameters in case output is cut in advanceI don't know.
R1 Details of how the model was used API Guide# I don't know what I'm talking about #https://api-docs.deepseek.com/zh-cn/guides/reasoning_modelI don't know.
This one. R1 UpdatedI'm not sure what you're talking about.Official websiteI don't know what you're talking about.AppletI don't know what you're talking about.App End and API in which the background length of the model remains 64KI don't know.If the user needs a longer context lengthI'm not sure what you're talking about.The context length can be called through other third-party platforms as 128K Open Source Version R1-0528 ModelI don't know.

## **Model Open Source**Model Open Source**‚Äã**‚Äã
DeepSeek-R1-0528 ‚ô™ With the old one ‚ô™ DeepSeek-R1 Use the same base ModelI'm not sure what you're talking about.Only post-training methods have been improvedI don't know.Pre-privatization deployment only needs to be updated checkpoint and tokenizer_config.jsonI don't know what you're talking about.tool calls Related changesI'm sorry, I don't know.Model Parameter is 685BI don't know what you're talking about.of which 14B Yes. MTP LayerThis is the first time I've ever seen you.Open Source Context Length 128KI don't know what you're talking about.WebendI don't know what you're talking about.App and API Provision 64K ContextI'm sorry, I don't know.
DeepSeek-R1-0528 Please refer to model weight downloads# I don't know what I'm talking about #
**Model Scope:**Model Scope: https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528
**Huggingface:**Huggingface: https://huggingface.co/deepseek-ai/DeepSeek-R1-0528
with the old version DeepSeek-R1 ConsistencyI'm not sure what you're talking about.This time, our open-source warehouse.I don't know what you're talking about.Including model weightsI'm not sure what I'm talking about.It's still uniform. MIT LicenseI'm not sure what you're talking about.and allow the user to use the model for outputI don't know what you're talking about.Train other models, for example, through model distillation.I don't know.

## **Latest callDeepseek-R1-0528 API Website**Latest callDeepseek-R1-0528 API Website
1I don't know what you're talking about.Openrouter Address# I don't know what I'm talking about #https://openrouter.ai
2. GMI tutt: https://inference-engine.gmicloud.ai
3. Novita tutt: https://novita.ai
4. Nebius tbtit: https://studio.nebius.com
5. Inference toti: https://inference.net 
Official presentation# I don't know what I'm talking about #https://api-docs.deepseek.com/zh-cn/news/news250528
