---
layout: post
title: ComfyUI Launch Original API Node
date: 2025-06-11 12:00:00 +0800
category: Frontier Trends
thumbnail: /style/image/comfyui-api-api_1.jpg
icon: chat
---
* content
{:toc}

ComfyUI launched **Native API Nodes**, which allows users to ** call multiple pay-as-you-go models API** directly in the workflow, e.g. Google Veo2, OpenAI GPT-4o image, Stability AI, Luma, Recraft, Pika 2.2, PixVerse, Ideogram etc. **11 model series, 65 nodes**. This means that users can call different models, such as images, video generation, text-to-videos, in parallel with the same ComfyUI process,** without the need to exit the interface to organize the generation of tasks.** In addition, ComfyUI also supports:

- ** Users bring their own API key** (reuse existing platform accounts);

- ** Parallel execution of API request** (accelerated multi-model call speed);

- ** First-time support for VIDEO type** (original video generation task);

- **new UI visual brand upgrade** (in the 1990s, moving + Y2K technology wind);

All API node functions are ** optional** and the ComfyUI body is still ** completely free of charge**.

#ComfyUI Original API Node? ComfyUI used to be a graphical node-driven local image-generation tool that was widely used in the Stable Diffusion community. But this update introduced a brand-new component: **Native API Nodes**: let the user in the ComfyUI ** direct the use of several mainstream "External Pay Large Model API"** without having to leave the interface or write a code. This means that:** you can mix multiple business models (image/ video/multimodel) in a workflow **, unify dispatching and generating, and significantly increasing the efficiency of creation.

# The currently supported model and API at ComfyUI add the following new raw access support for **11 model series**, totalling **65 nodes**, covering multi-model tasks such as image, video, text-to-video, image-to-video.

# Support models include: [] [https://assets-v2.circle.so/fgk399zgqypd1otehvj7m3scovy5][] [https://assets-v2.circle.so/djh987lxsny63szj5i4t73g3n0k]  The user can call the model by dragging and dropping these nodes without having to log separately into each platform to support the combination call.

# Details of how to use the workflow

##Video API Select template to run directly [] (https://assets-v2.circle.so/w3dmoa9s88gak2vnoqi0mqvt0] [https://assets-v2.circle.so/cfgllq6zmy34yiq08kfnd)] (https://assets-v2.circle.so/k37s4gj8nj6onjujkk) [symbn/Keykäkätäk)

# Supported parallel execution: When calling multiple external APIs (e.g. image+video+subtitles), ComfyUI automatically executes different nodes in parallel, significantly increasing overall production speed. Examples of multi-model collaboration:

- Text  GPT-Image Generate Person sketches

Luma Photon, plus real sense of light.

- Scene + Prompt  PixVerse Generate Video Snippets

- Video + Description Pika Generate Animated White Camera

~ (https://assets-v2.circle.so/3rx51q3s60o55mkr9xu5njpwr) # first-time introduction of VIDEO type birth support for ComfyUI first-time support of “video generation” type nodes (VIDO type) meaning:

- It is no longer limited to image missions;

- Support for video-image combination processes;

- To support cross-model collaboration (e.g. image-to-video, video-generation re-engineering of segments, etc.);

- laying the foundation for the next audio/3D extension.

# The new upgrade of visual brands, in addition to the technological upgrades, ComfyUI has officially launched a brand-new visual system:

- **Logo:** consists of several modules that respond to the node graphical workflow;

- ** fonts and colours: ** joined 90-year kinetic style + Y2K digital sense;

- ** Concept expression: ** Retains the “free, accessible, hackeric” community spirit, emphasizing that instrumentality coexists with creative openness.[1] (https://assets-v2.circle.so/5zfpponn56jnnchf08pte3hppn295m) Official words: “We want to convey that ComfyUI is still free and open. It has also become a powerful tool that can be used in the production process.”

# # Several summary points of value 1. ** ** Unified multi-model access platform** integrates multiple mainstream API calls in a workflow and no longer needs to switch platforms, manually upload download materials. ** ** Free body + selected commercial API** ComfyUI main body remains open, free of charge, API node is used on demand. Both the creator's freedom and the business-level modeling capability are preserved. ** Real multi-model generation tubes for creators ** Photos, videos, text, structural data, styles are integrated and it will be possible to add audio/3D node in the future.

# Who's fit for these functions?

- creators of graphic content (e.g. microblogging, public numbers, short video drawings)

- AI Video Editor (Users of Pika, Veo2, PixVerse)

- Stable Diffusion Advanced User (wanting to mix other models)

- Designer/advertisement creator (image mix generation)

- Educational content developer (video transfer, video with AI animation)

- Product manager/developer (building AI workflow prototype)

Official presentation: https://blog.comfy.org/p/comfyui-native-api-nodes