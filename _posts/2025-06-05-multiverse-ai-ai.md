---
layout: post
title: Multiverse: The first multi-person online game generated by AI The behavior of players affects the AI simulation world in real time
date: 2025-06-05 12:00:00 +0800
category: Frontier Trends
thumbnail: /style/image/multiverse-ai-ai_1.jpg
icon: book
---
* content
{:toc}

Enigma Labs launched Multiverse, a ground-breaking AI-generated multi-player game that allows players to build and influence the world together in real time. The model aims to address the limitations of existing world models that can only simulate a “single subject” and construct structures that support **multi-agent perception, prediction, and interaction in a shared environment.

- ** Multiple interactions**: multiple players can enter this world that is simulated by AI at the same time, and ** everyone's behavior affects the world in real time. ** For example, you accelerate, drift, overcarriage, and make the world change.

- ** Cost-effective**: They spent less than **$1,500** to complete training and research and development and can run on regular PCs. The secret is not a lot of math, but technological innovation.

- ** Complete open source**: they open up the entire project to code, data, model parameters, architecture, research results so that anyone can learn, use or improve.

- ** Use is not limited to games**: this “manual world model” means more than games, but can also be applied in a broader simulation environment, such as robotic training, AI collaboration, etc.

# The core technical architecture details

## Baseline, a traditional single-person world model, usually consists of three parts: **Action Encoder** converts player input (e.g. keys) into vector expression. ** Denoising Network** is based on Diffusion models, using input actions and previous videos to predict the next frame. ** Up-sampler** enhances low-resolution images to generate high-resolution output.

##Multiverse: The structural innovation of multi-person models has been redesigned by Multiverse to fit multiple scenes:

- ** Double input action encoder**: to receive control input from both players and to construct a combined vector.

- ** Shared Noise Network**: In a unified model, create video frames under two player perspectives and ensure time and space consistency.

- ** Parallel top sampler**: two low-resolution frames are sampled separately, but consistent image style and dynamic information.

(https://assets-v2.circle.so/80znrdzd1n63gccprglqjzf18nku) ** Critical ** In a multi-faceted scene, AI must generate two visions at the same time, but the content must be the same. ** That is, ** When you see the crash, I must also see it **.

#  Entering data structure design: the way the visual perspective is integrated In order to achieve shared perception, the team has tried two visual integration methods: ** Vertical stacking (Split-screen)**: As with the traditional partition game, two images are combined up and down. ** Channel axis integration**: stacking two images along the RGB channel dimensions (i.e., turning each pixel into six channels)[] (https://assets-v2.circle.so/o80dy5soppsqijflf8mxg84uj9p) shows that: ** Channel axis integration is better** because this way allows the U-Net network to process information from both angles at the same time in all volume layers to enhance the logical consistency of the picture.

#  Long time sequence modelling and context optimization

# # Question: Multi-car interactive physical phenomena (e.g. relative movement, brake effects)** slowly changing but critical** require longer windows.

- ** Short-term dynamics (e.g. brakes)**: enough modelling with 8 frames (0.25 seconds).

- ** Long-term interactions (e.g., overcarriage, collisions)**: 0.5 to 1 second or even longer span is required.

## Solution: ** Rare time sampling + tier prediction **

- Use the most recent 4 frames + take 1 frame per 4 frames (8 frames in total) to get a longer sense without increasing the visible pressure.

- Introduction of **Curriculum Learning (course-based training)**: training for short-term predictions leading to a gradual transition to a maximum of 15 seconds of projection, balancing learning efficiency with expression skills.

#  Data set construction and training process

# Game platform selection: Gran Turismo 4 (GT4)

- Use of the Tsukuba Circuit scene to facilitate modelling and recurrence.

- Game origin does not support 1v1 mode, team by **reverse engineering** forced start of real 1v1.

# # # data acquisition strategy

- Each game records a double viewing (per player's perspective), synthesizing at a later stage.

- Controlling input (barrells, brakes, diversions) to read HUD elements** via a computer visual reading, without manual recording.

# # Automation generation mechanism

- Generate available video data using the B-Spec mode (AI driver)+Script to send control command  batches in GT4.

- Experimental access to the OpenPilot autopilot model is controlled, but ultimately B-Spec is selected for stability and resource optimization.

# Research values and application prospects

# Core breakthrough:

- Modelling of a world of coherence in a multi-faceted perspective** (synchronized multi-perspective development).

- Introduction of efficient training strategies and replicable data pipelines to support the opening up of community recovery and validation.

## Potential applications: [] (https://assets-v2.Circle.so/Rolivejo1gd7ku1747rkbzxfwfbh) Official presentation: https://enigma-labs.io/blogGitHub:https://github.com/EnigmaLabsAI/multiverse model: https://huggingface.co/Enigma-AI