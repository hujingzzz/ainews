---
layout: post
title: Artificial Anlys: DeepSeek leaps to the second largest AI laboratory in the world, alongside Google, to secure the leadership of the Open Source Model
date: 2025-06-02 12:00:00 +0800
category: Frontier Trends
thumbnail: /style/image/artificialanlys-deepseek-ai-google_1.jpg
icon: note
---
* content
{:toc}

@ArtificialAnlys post-training update R1-0528 of the model R1 just released by DeepSeek has significantly increased the model’s performance in a number of smart assessments, up to 68 points, and is ranked second in the world with Google Gemini 2.5 Pro, after the OpenAI series. This update marks the second largest artificial intelligence laboratory in the world.

## The R1 of DeepSeek, published in May 29, goes beyond xAI, Meta and Anthropic to become the world’s second largest AI laboratory with Google, while sitting on the head of the open-source heavyweight model.  (https://assets-v2.circle.so/785r9w5d3a67sk2jl9patsnd) The R1 goes beyond xAI, Meta and Anthropic to become the world’s second largest AI laboratory. # #

- Comprehensive upgrading of intelligence: this is particularly evident in AIME 2024 (competing mathematics, +21 points), LiveCodeBench (code generation, +15 points), GPQA Diamond (science reasoning, +10 points), Humanity's Last Exam (sense and knowledge, +6 points).

- No change in structure: R1-0528 is a post-training update that does not change the V3/R1 architecture - still a super-large model with a total 671B parameter (37B active parameter).

- Significant increase in programming capacity: R1 is now at the same level as Gemini 2.5 Pro in the manual analysis of programming indices and lags behind only o4-mini (high match) and o3.

-  Token Use has increased significantly: R1-0528 used 99 million tokens in the assessment, more than 40% more than the original R1 71 million. That is, the new version, “Thougher Thinking.” This is still not the highest, but Gemini 2.5 Pro uses 30% more token than R1-0528.[3] (https://assets-v2.circle.so/e40vlyqua7z62co7417jynhaz1) ** Some conclusions on AI: **

- The gap between open-source and closed-source models has narrowed unprecedentedly: open-source weight models can still maintain smart growth comparable to proprietary models. DeepSeek R1 has returned to the same position since January, when he first took the second largest position in the world.

-  The Chinese-American technological axiom: The model of the AI lab in China has almost evened the American counterpart. This release continues this trend again. DeepSeek is now leading the AI smartness index of Anthropic and Meta.

- Strengthening learning for intellectual uplifting: DeepSeek, without changing its structure and pre-training, has significantly upgraded its intelligence after training (especially intensive learning) and validated the key role of RL in the reasoning model. OpenAI has expanded its calculation of RL by 10 times between o1 and o3, while DeepSeek has demonstrated its ability to keep up with this rhythm. The expansion of RL requires fewer and more efficient computing resources than pre-training, and benefits AI laboratories with limited resources.[3] (https://assets-v2.circle.so/oi8dus0jw2vvvjgy2okw0c6cscff) Detailed report: https://artificialanalis.ai/models/deepseek-r1/providers]