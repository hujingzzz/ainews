---
layout: post
title: MiniMax Release the mixed attention reasoning open source model. MiniMax-M1 Training costs only. 53.5 United States dollars Performance approaching top closed source model
date: 2025-06-06 12:00:00 +0800
category: Frontier Trends
thumbnail: /style/image/minimax-minimax-m1-53-5_1.jpg
icon: note
---
* content
{:toc}

MiniMax Release **MiniMax-M1 **MiniMax-M1 First Global**Open Source**Open SourceIt's... it's... it's...**Models of large-scale mixed attention reasoning language**Models of large-scale mixed attention reasoning languageI don't know.The main feature is integration. **Mixed expert structureMoEI'm not sure what I'm talking about.**Mixed expert structureMoEI'm not sure what I'm talking about. And efficient. **Lightning Attention Mechanisms**Lightning Attention Mechanisms It's the speed of reasoning.Significant advantages in long text processing and complex mission performanceI don't know.
On most missions. MiniMax-M1 **It's much better than other open source models.Like Qwen3DeepSeek-R1I'm not sure what I'm talking about.**It's much better than other open source models.Like Qwen3DeepSeek-R1I'm not sure what I'm talking about. And it's closing up even partially beyond the commercial closed-source model.I don't know.
![](https://assets-v2.circle.so/1f5mbmzuq76ohtvlbhfnh36puu1t)The model is based on the pre-generation model. **MiniMax-Text-01**MiniMax-Text-01 Development Total parameter size is **4560Billions**4560Billions Every one.tokenActivate Parameter As **45.9Billions**45.9Billions Maximum support **100Milliontokens**100Milliontokens context inputAbout DeepSeek R1 It's... it's... it's...8MultiplyI'm sorry, I don't know.
![](https://assets-v2.circle.so/ffdi8r1a1kq2do0llguv07zzpsx1)
### Model version
MiniMax-M1-40K- Context-100Million Logic budget-tokens40K
MiniMax-M1-80K- Context-100Million Logic budget-tokens80K

### **Mixed expert modelMixture-of-Experts, MoEI'm not sure what I'm talking about.**Mixed expert modelMixture-of-Experts, MoEI'm not sure what I'm talking about.

- Every one. token Activate the contract 45.9 Billion parametersTotal Parameters 4560 BillionsThis is the first time I've ever seen you.Call only some experts Improving the efficiency of reasoningI don't know.

- It's even.‚ÄúAt the end of the day,Large modeling capacityIt's not a good idea, it's not a good idea.and‚ÄúAt the end of the day,Landable deploymentIt's not a good idea, it's not a good idea.The paradox.I don't know.

### **Lightning Attention Mechanisms**Lightning Attention Mechanisms

- It's a kind of thing.**Large-scale context**Large-scale contextOptimizing the way to focus.I don't know.

- Compare DeepSeek R1 Waiting for a model I'm dealing with it. 10 Million tokens _Other Organiser MiniMax-M1 the calculation is only based on 25%I don't know.

### **Super-long context treatment**Super-long context treatment

- **Native support 100 Million token Context**Native support 100 Million token Context More than most of these models.For example... DeepSeek R1 Support 128KI'm sorry, I don't know.

### Training costs

- **RL All I need is intensive training. 3 Week + 512 H800 GPU**RL All I need is intensive training. 3 Week + 512 H800 GPU

- **Total cost only 53.5 United States dollars**Total cost only 53.5 United States dollars

## MiniMax-M1 Performance
![](https://assets-v2.circle.so/xnnendtc6s9lodor6ie8rqa05joh)
### 


![](https://assets-v2.circle.so/mvnopfctmexh5yg8kshk0inqxc2h)
### ‚úÖ 1. **Mathematics and logical reasoning**Mathematics and logical reasoning

- Yes. AIME 2024 High scores on the competition. **86.0%**86.0%

- Yes. MATH-500 Close to the full point.96.8%I'm not sure what I'm talking about.

- Shows good chain thinking.Chain-of-ThoughtI'm not sure what I'm talking about.Capacity

- SFT + RL Phase targeting strengthens the path of reflective reasoning.

### ‚úÖ 2. **Universal and advanced programming tasks**Universal and advanced programming tasks

- Overwrite arithmetic programming themeLiveCodeBenchI'm not sure what I'm talking about.To Multimodule Engineering TasksFullStackBenchI'm not sure what I'm talking about.

- Show syntax for codeComprehensive understanding of the logical structure

- It's stable. Suitable for code generation or intelligenceIDEIntegration

### ‚úÖ 3. **Real software engineering tasks**Real software engineering tasks

- SWE-bench-It's based on the truth. GitHub Problem Verify whether the model can be completed automatically bug Repair and PR Submit

- MiniMax-M1 The real sandbox system was built. And authenticate at the code enforcement level.

- Score **56%**56% More than all open source models Second only to the latest closed-source model

### ‚úÖ 4. **UltraLong Text Capability**UltraLong Text Capability

- Support **100Million tokens Context**100Million tokens ContextNative supportI'm not sure what I'm talking about.

- Yes. MRCRLongBench When the mission's done well,-
**MRCR-128K Score 73.4%**MRCR-128K Score 73.4%-By comparison GPT-4 It's closer to real understanding.

- It handles complex instructions.Legal instrumentsLong content of scientific documents

### ‚úÖ 5. **Agent Capacity-Tools Use and Call**Agent Capacity-Tools Use and Call

- TAU-bench Simulate Truth API Use scenes

- MiniMax-M1 Beyond Gemini 2.5 and Claude 4-
Airline-62%

- Retail-63.5%

It shows that it's complicated.+There's a great deal of adaptive capacity in the action-calling smart-body task.

### ‚úÖ 6. **Dialogue and assistant capacity**Dialogue and assistant capacity

- MultiChallenge Score 44.7%

- and Claude 4DeepSeek-R1 Pair

- Stabilizing in multi-mission dialogues Fits for assistant base model

### ‚ö†Ô∏è 7. **Weaknesses-Less able to answer questions and answers about the facts.**Weaknesses-Less able to answer questions and answers about the facts.

- Yes. SimpleQA ‚ô™ Up there ‚ô™ 18.5% Annotations-
Yeah, it's short.Make sure the answer to the question is accurate and there's room for improvement.

- Related to training data distribution or incentive model preferences

## MiniMax-M1 Technological innovations and bright spots

### üîß Structural innovation-**Mixed attention mechanismHybrid AttentionI'm not sure what I'm talking about.**Mixed attention mechanismHybrid AttentionI'm not sure what I'm talking about.
üîπ 1. **Lightning Attention + Softmax Attention**Lightning Attention + Softmax Attention

- **Lightning Attention**Lightning Attention It's a kind of... **Linear Complexity Attention Mechanism**Linear Complexity Attention Mechanism Alternative traditions quadratic attention Calculating is slower as the length of the text increasesI don't know.

- Every 7 Layer Lightning Attention Insert 1 Layer Softmax Attention To enhance contextual modelling capabilitiesI don't know.

- Advantages-
Support the long-term. **1,000,000 tokens context input**1,000,000 tokens context input

- Substantial reduction in the number of reasoning calculationsGenerate, for example. 100K tokens Hour FLOPs Only DeepSeek R1 It's... it's... it's... 25%I'm not sure what I'm talking about.

### üß† Model size and computational efficiency-**Mixture of ExpertsMoEI'm not sure what I'm talking about.Mixed Expert Mechanism**Mixture of ExpertsMoEI'm not sure what I'm talking about.Mixed Expert Mechanism

- Total model parameters reached **4560 Billions**4560 Billions Every time only activates **45.9 Billions**45.9 BillionsThat's what I'm talking about. 10%I'm not sure what I'm talking about.

- Used **32Expert module**32Expert module Activate each input selection part

- Advantages-
**Calculating efficient**Calculating efficient-Without loss of capacity Use only some parameters for reasoning **Significant reduction in reasoning and training costs**Significant reduction in reasoning and training costs

- **Strong scalability**Strong scalability-Total parameters can be extended to hundreds of billions of grades without compromising the cost of useI don't know.

- **It's fine. It's fine.**It's fine. It's fine.-Optimize local experts only It's also good for the field.I don't know.

- Keep the big model up. It also applies to multitasking.

### üß™ Enhanced learning training optimization-**NewRLAlgorithms CISPO**NewRLAlgorithms CISPO
üîπ Problem-Traditional methods such as: PPOGRPO Existence token Scissors. It ignores the key turning point in the reasoning.Like‚ÄúAt the end of the day,Wait.Think again.I'm sorry, I'm sorry, I'm sorry, I'm sorry.
üîπ Solutions-MiniMax Presented **CISPOClipped IS-weight Policy OptimizationI'm not sure what I'm talking about.**CISPOClipped IS-weight Policy OptimizationI'm not sure what I'm talking about.

- Amend to read‚ÄúAt the end of the day,Cut sample weightsI'm not sure what I'm going to do.**Keep All tokens The training signal.**Keep All tokens The training signal.

- Advantages-
Keep a rare but important line of reasoning

- Training is more stable. More efficient.

- ‚ô™ In and out ‚ô™ GRPODAPO The speed of training increases in the comparison. **2Multiply**2Multiply

### üìà A complete increase in reasoning.-**It's an advantage in a number of complex tasks.**It's an advantage in a number of complex tasks.

- **Ultra-Long Text Processing**Ultra-Long Text Processing-Support 1M Input80K Output Appropriate for scientific dissertationsScenes such as legal instruments

- **Complex mission reasoning**Complex mission reasoning-Yes. AIME MathematicsLiveCodeBench ProgrammingSWE-bench Excellent performance in software engineering tasks

- **Tool Callability**Tool Callability-Yes. TAU-bench Beyond Gemini 2.5 ProClaude 4 Fits to build complex intelligence bodies

### üß∞ Optimization of engineering efficiency

- **RL All we need is training. 3 Week + 512 H800 GPU**RL All we need is training. 3 Week + 512 H800 GPU

- **Total cost only 53.5 United States dollars**Total cost only 53.5 United States dollars Largely below average GPT-4 Waiting for closed-source model training costs

- **Logic and training have cost advantages.**Logic and training have cost advantages. It helps model landing and universalization.

### üì¶Native Function Calling + Tool Use Capacity
‚úÖ Innovation point-

- built-in function callFunction CallingI'm not sure what I'm talking about.Modules Support output structured call parametersI don't know.

- Target Agent A complete tool call evaluation was appliedTAU-benchI'm not sure what I'm talking about.

üîç Advantages-

- No extra fine-tuning. Models can identify when to call Generate parameter formatting;

- Support building search enhancementAgentTask AssistantAPIInteractive robots.I don't know.

GitHub-https://github.com/MiniMax-AI/MiniMax-M1
Model-https://huggingface.co/collections/MiniMaxAI/minimax-m1-68502ad9634ec0eeac8cf094
Papers-https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf 
Online experience-https://chat.minimax.io/
